{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPERTY_ATTRIBS = ['Age', 'Area', 'Backup Water', 'Bathroom', 'Bathroom 1', 'Bathroom 2',\n",
    "       'Bathrooms', 'Bedroom', 'Bedroom 1', 'Bedroom 2', 'Bedroom 3',\n",
    "       'Bedroom 4', 'Bedrooms', 'Deposit Requirements', 'Description',\n",
    "       'Dining Rooms', 'Erf Size', 'Facing', 'Floor Number', 'Floor Size',\n",
    "       'Furnished', 'Garage', 'Garages', 'Garden', 'Gardens', 'Generator',\n",
    "       'Internet Access', 'Kitchen', 'Kitchens', 'Lease Period', 'Levies',\n",
    "       'Lifestyle', 'Lounge', 'Lounges', 'Nearby Public Transport',\n",
    "       'Number of floors', 'Occupation Date', 'Outbuilding', 'Parking',\n",
    "       'Pets Allowed', 'Pool', 'Pools', 'Rates and Taxes', 'Reception Rooms',\n",
    "       'Roof', 'Secure Parking', 'Security', 'Security 1', 'Security 2',\n",
    "       'Special Feature', 'Special Feature 1', 'Special Feature 2',\n",
    "       'Special Feature 3', 'Special Features', 'Special Levy',\n",
    "       'Standalone Building', 'Street Address', 'Style', 'Type of Property',\n",
    "       'Wall', 'Wheelchair Accessible', 'Window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scraped_data(links_list):\n",
    "    # Take in the list of scraped links to all of the suburbs on Prop24 and then extract the price data\n",
    "    # Returns a dictionary with the suburb price data by suburb\n",
    "    suburb_price_data = {}\n",
    "    for link in links_list:\n",
    "        # Open up each link\n",
    "        base_url = 'https://www.property24.com/' + str(link)\n",
    "        response = requests.get(base_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the year and price data using the custom function\n",
    "        year, price = extract_prices(soup)\n",
    "        suburb = link.split('/')[2].replace('-',' ').title()\n",
    "        \n",
    "        # Place the data into a dictionary with {Suburb: {Year: Price}} pairing\n",
    "        suburb_price_data[suburb] = dict(zip(year,price))\n",
    "    return suburb_price_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "for link in links:\n",
    "    address = link.get('href')\n",
    "    \n",
    "    # This filters out the links so we're only looking at suburbs links (and some houses, which are filtered out)\n",
    "    if re.search('^/for-sale', address) and re.search('[0-9]{4,5}$', address):\n",
    "        temp_address = address\n",
    "        \n",
    "        # This removes links to individual properties\n",
    "        if(len(temp_address.split('/')[-1])) < 6:\n",
    "            link_list.append(address)\n",
    "link_list = list(set(link_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_price_data = {}\n",
    "suburb_price_data = load_scraped_data(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_df = pd.DataFrame(suburb_price_data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Walmer Estate', 'Marina Da Gama', 'Kenwyn', 'Rosebank', 'Belhar',\n",
       "       'Newfields', 'Valhalla Park', 'Kirstenhof', 'Newlands Upper', 'Montana',\n",
       "       ...\n",
       "       'Kenilworth Upper', 'Higgovale', 'Heathfield', 'Bergvliet', 'Woodstock',\n",
       "       'Sybrand Park', 'Clifton', 'Cape Farms', 'Lansdowne', 'Surrey Estate'],\n",
       "      dtype='object', length=136)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suburb_df.index =suburb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_df.to_csv(\"property_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prices(soup_data):\n",
    "    # Function takes a document processed with Beautiful soup and extracts the year and rand-value of property on the page\n",
    "    # Returns a two lists with the year and price data\n",
    "    \n",
    "    jsdata = soup_data.findAll('script')\n",
    "    price_by_year = {}\n",
    "    year, price = [], []\n",
    "    # Split the text by \"\" and then use regex to extract the years and rand amounts\n",
    "    all_text = str(jsdata).split('\"')\n",
    "    for word in all_text:\n",
    "        # Find the date\n",
    "        if re.search(\"^:20\", word) and len(word) < 7:\n",
    "            year.append(word[1:-1])\n",
    "\n",
    "        # Find the rand amount and process it to get rid of weird formatting\n",
    "        if re.search(\"^R [0-9]\", word):\n",
    "            price.append(int(re.sub(\"\\D\",\"\",str(word[2:]))))\n",
    "    \n",
    "    return year, price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pages(num_pages):\n",
    "    '''\n",
    "    Description: Function that scrapes webpages on property24 to create a list of links to search further\n",
    "    Arguments: num_pages - the number of pages to scrape\n",
    "    Returns:   a list with the links for all of the properties\n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://www.property24.com/apartments-to-rent/cape-town/western-cape/432'\n",
    "    search_terms = '?sp=pf%3d3500%26pt%3d7000'\n",
    "    properties = []\n",
    "    \n",
    "    # Append the URL according to the formatting rules for the URL\n",
    "    for it in range(num_pages):\n",
    "        if it == 0:\n",
    "            page_var = ''\n",
    "        else:\n",
    "            page_var = '/p' + str(it+1)\n",
    "        scrape_url =  str(base_url) + str(page_var) + str(search_terms)\n",
    "        \n",
    "        # This try/except allows the function to avoid errors if too many pages are passed to it\n",
    "        try: \n",
    "            response = requests.get(scrape_url)\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        # Extract the text from the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the links and filter out ones that aren't useful\n",
    "        page_links = soup.findAll('a')\n",
    "        for link in page_links:\n",
    "            address = link.get('href')\n",
    "            if address[1:8] == 'to-rent' and '%' not in address and len(address) > 8:\n",
    "                properties.append('https://www.property24.com/' + str(address))\n",
    "    return properties\n",
    "\n",
    "def extract_property_data(properties, property_dict, property_attribs):\n",
    "    '''\n",
    "    Description: function that extracts key data points from the property links\n",
    "    Arguments: properties - a list containing all of the links to the properties scraped from the site\n",
    "    Returns:   a dictionary with the selected data points from the properties\n",
    "    '''\n",
    "    #property_dict = {}\n",
    "    if len(property_dict.keys()) > 0:\n",
    "        i = len(property_dict.keys())\n",
    "    else:\n",
    "        i = 0\n",
    "    \n",
    "    for link in properties:\n",
    "        # setup a blank dictionary with the same attributes\n",
    "        props = dict((attr, 0) for attr in property_attribs)\n",
    "        # Extract the text from the page\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "        try:\n",
    "            # Takes the area from part of the link\n",
    "            props['Area'] = link.split('/')[5]\n",
    "            \n",
    "            # Finds all the attributes data and stores it in a dictionary\n",
    "            data = soup.findAll('div', attrs = {'class':'row p24_propertyOverviewRow'})\n",
    "            for item in data:\n",
    "                items = item.text.strip().replace('\\n\\n','|').split('|')\n",
    "                props[items[0]] = items[1]\n",
    "\n",
    "            write_up = soup.find('span', attrs = {'class':'p24_dPL js_readMoreText'}).text.replace('\\n','')    \n",
    "            props['Description'] = write_up\n",
    "            props['Link'] = link\n",
    "            \n",
    "            price = soup.find('span', attrs = {'class':'p24_price'}).text\n",
    "            \n",
    "            # Cleans the price data; repalces a text field called POA\n",
    "            if price == \"POA\":\n",
    "                price = 0\n",
    "            else:\n",
    "                price = price.replace('R ','').replace('&#160;','') \n",
    "            props['Price'] = price\n",
    "\n",
    "            property_dict[i] = props\n",
    "            i += 1\n",
    "        except:\n",
    "            continue        \n",
    "\n",
    "    return property_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_dict = {}\n",
    "property_dict = extract_property_data(properties, property_dict, PROPERTY_ATTRIBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#property_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Createa dataframe and then remove columns where 80% of the entries are 0\n",
    "df = pd.DataFrame(property_dict, columns = property_dict.keys()).transpose()\n",
    "for column in df.columns:\n",
    "    a = (df[column].values == 0)\n",
    "    a = np.sum(a*10)\n",
    "    #print(a)\n",
    "    if (a > 8*len(df[column].values)):\n",
    "        df.drop(columns = [column], inplace=True)\n",
    "\n",
    "items = []        \n",
    "for item in df['Price']:\n",
    "    if type(item) == int:\n",
    "        items.append(item)\n",
    "    else:\n",
    "        items.append(np.int(item.replace('\\xa0','')))\n",
    "df['Price'] = items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = df.to_csv('Houses.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
